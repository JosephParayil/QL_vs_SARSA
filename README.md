Title: Carelessness Over Caution: Understanding the Hidden Power Behind Off-Policy Reinforcement Learning Algorithms

Abstract:
This project investigated reinforcement learning algorithms QL (off-policy) and SARSA (on-policy) in order to derive a computational explanation for an intriguing performance phenomenon. The only difference between the two is that QL assumes that ε-probability exploration does not happen (although it exists), whereas SARSA simply accounts for it. Yet it is observed that QL significantly outperforms SARSA in many training examples. The goal of this project was to shed light on this phenomenon first by statistical experimentation and then by mathematical analysis. For the main experiment, the correlation between environment reward sparsity and the average convergence rates of QL and SARSA was tested. This involved a complex process of generating and evaluating 79 SimpleGrid environments that varied in controlled ways with respect to shortest path length and average number of adjacent walls. A moderate, statistically significant correlation between reward sparsity and the QL-SARSA performance difference was found. The subsequent mathematical analysis aimed to develop a proof behind the existence of training instances where QL beats SARSA. It was determined that SARSA's accounting of ε-probability exploration's costs prevented further exploration of the environment, causing it to converge at a suboptimal policy. QL, and by extension, its more powerful variants, do not have this problem. This project concludes that off-policy algorithms outcompete on-policy ones when the agent needs to persevere through a costly exploration phase to achieve long-term rewards. Indeed, pushing through the perils of exploring the unknown can eventually prove to reward itself many times over.
